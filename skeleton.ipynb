{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skeleton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenjune17/Neural-Machine-Translation-English-Tamil-model/blob/master/skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmjh290raIky",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural Machine Translation with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSNimO-jpJb",
        "colab_type": "code",
        "outputId": "d63ab224-b999-416f-8737-ddd56d38a712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install -U tf-nightly-gpu\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "from math import log\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.4)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n",
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/4d/75a1a93bf7971f4f6d50071ad11885c3a0d480fa7709f0e065b3b62686e7/tf_nightly_gpu-1.15.0.dev20190628-cp36-cp36m-manylinux1_x86_64.whl (396.1MB)\n",
            "\u001b[K     |████████████████████████████████| 396.1MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/60/afa129c3621d62c885599076f8e89737d66e5dfffad1a08842b1c11b4540/tb_nightly-1.14.0a20190614-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2)\n",
            "Collecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/07/527b55522ad7c3b8b22ee00ba08d23057de506c465c94c8ac48dc7a1155b/tf_estimator_nightly-1.14.0.dev2019070201-py2.py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.4)\n",
            "Collecting opt-einsum>=2.3.2 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu) (2.8.0)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, opt-einsum, tf-nightly-gpu\n",
            "Successfully installed opt-einsum-2.3.2 tb-nightly-1.14.0a20190614 tf-estimator-nightly-1.14.0.dev2019070201 tf-nightly-gpu-1.15.0.dev20190628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "#### Inspired by \n",
        "https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb\n",
        "\n",
        "The dataset is downloaded from\n",
        "\n",
        "a) github.com/achchuthany/En-Ta-Parallel-Corpus\n",
        "\n",
        "b) github.com/joshua-decoder/indian-parallel-corpora\n",
        "\n",
        "c) OPUS\n",
        "\n",
        "\n",
        "\n",
        "### Goal\n",
        "Convert Tamil sentences to english sentences using a Basic Seq2Seq with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3KAm8f5VZik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a10687b-ec3b-4346-e6d5-faee812f49c5"
      },
      "source": [
        "# Download the file\n",
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "\n",
        "#path_to_file = '../content/en_tam_parrallel_text_nodups_sorted_eng_tam_reversed.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9TqHPL_USJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(path_to_file):\n",
        "  !unzip en_tam_parrallel_text_nodups_sorted_eng_tam_reversed.v1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "# # Converts the unicode file to ascii\n",
        "# def unicode_to_ascii(s):\n",
        "#     return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "#         if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "# def preprocess_sentence(w,language='tamil'):\n",
        "    \n",
        "#     if language=='english':\n",
        "#       w = unicode_to_ascii(w.lower().strip())\n",
        "#       w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "#       w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#       w = re.sub(r\"[^a-zA-Z?.'!]+\", \" \", w)\n",
        "#     else:\n",
        "#       w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w.strip())\n",
        "#       w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#       w = re.sub('[a-zA-Z?.!]+', \" \", w) #replace english alphabets in tamil sentence\n",
        "#     w = w.rstrip().strip()\n",
        "#     w = '<start> ' + w + ' <end>'\n",
        "#     return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLkTQyueGEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opI2GzOt479E",
        "colab": {}
      },
      "source": [
        "# en_sentence = u\"Everybody hide!\"\n",
        "# ta_sentence = u\"எல்லோரும் மறைக்க!\"\n",
        "\n",
        "# print(preprocess_sentence(ta_sentence))\n",
        "# print(preprocess_sentence(en_sentence,'english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# # 1. Remove the accents\n",
        "# # 2. Clean the sentences\n",
        "# # 3. Return word pairs in the format: [ENGLISH, TAMIL]\n",
        "# def create_dataset(path,start,stop):\n",
        "#   ex1=[]\n",
        "#   ex2=[]\n",
        "#   lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "#   for l in lines[start:stop]:\n",
        "    \n",
        "#     eng,ta= (l.split('\\t'))\n",
        "#     ex1.append(preprocess_sentence(eng,language='english'))\n",
        "#     ex2.append(preprocess_sentence(ta,language='tamil'))\n",
        "    \n",
        "\n",
        "#   return zip(*list(zip(ex1,ex2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiioHgi0d1Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTbSbBz55QtF",
        "colab": {}
      },
      "source": [
        "#en,ta  = create_dataset(path_to_file, start=120000,stop=120090)\n",
        "\n",
        "#print(en[-1])\n",
        "#print(ta[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ms7_lsMd5QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70iQdGAxdiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_extra(input_tensor,target_tensor,threshold):\n",
        "  \n",
        "\n",
        "\n",
        "  new_input_tensor = []\n",
        "  new_target_tensor=[]\n",
        "  \n",
        "  for i,j in zip(input_tensor,target_tensor):\n",
        "    if (len(i) <= threshold) and (len(j) <= threshold):\n",
        "      new_input_tensor.append(i)\n",
        "      new_target_tensor.append(j)\n",
        "      \n",
        "    \n",
        "  return(new_input_tensor,new_target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etoJpagHePWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmMZQpdO60dt",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "# def tokenize(inp_lang,targ_lang):\n",
        "#   inp_lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "#       filters='')\n",
        "#   inp_lang_tokenizer.fit_on_texts(inp_lang)\n",
        "\n",
        "#   ip_tensor = inp_lang_tokenizer.texts_to_sequences(inp_lang)          #convert the words to numbers of varying lengths\n",
        "  \n",
        "#   targ_lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "#       filters='')\n",
        "#   targ_lang_tokenizer.fit_on_texts(targ_lang)\n",
        "\n",
        "#   tar_tensor = targ_lang_tokenizer.texts_to_sequences(targ_lang)          #convert the words to numbers of varying lengths\n",
        "\n",
        "  \n",
        "\n",
        "#   return ip_tensor,inp_lang_tokenizer,tar_tensor,targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "# def load_dataset(path, start,stop):\n",
        "#     # creating cleaned input, output pairs\n",
        "#     inp_lang,targ_lang  = create_dataset(path, start,stop)\n",
        "#     input_tensor, inp_lang_tokenizer, target_tensor, targ_lang_tokenizer = tokenize(inp_lang,targ_lang)\n",
        "#     unk_id = list(inp_lang_tokenizer.word_index.values())[-1] + 1\n",
        "#     inp_lang_tokenizer.word_index['<UNK>'] = unk_id\n",
        "#     inp_lang_tokenizer.index_word[unk_id]  = '<UNK>'\n",
        "\n",
        "#     return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer, unk_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBXDbTkBdsNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Spanish sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "    \n",
        "    # English sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# # Try experimenting with the size of that dataset\n",
        "\n",
        "# start_lines = 160970\n",
        "# stop_lines  = 228700\n",
        "# input_tensor, target_tensor, inp_lang, targ_lang,unk_id = load_dataset(path_to_file, start=start_lines,stop=stop_lines)\n",
        "\n",
        "# # Calculate max_length of the target tensors\n",
        "# prev_ip = len(input_tensor)\n",
        "# prev_tar = len(target_tensor)\n",
        "# input_tensor,target_tensor = remove_extra(input_tensor,target_tensor,threshold=16)\n",
        "# after_ip = len(input_tensor)\n",
        "# after_tar = len(target_tensor)\n",
        "\n",
        "# print('{} and {} sentences removed from ip and target tensor after filtering by threshold'.format((prev_ip-after_ip),(prev_tar-after_tar)))\n",
        "# input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,#set the length of the sequences to be same by padding zeros in the end\n",
        "#                                                         padding='post')\n",
        "# target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,#set the length of the sequences to be same by padding zeros in the end\n",
        "#                                                         padding='post')\n",
        "# max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "# print(max_length_targ)\n",
        "# print(max_length_inp)\n",
        "# print('Maximum length of input and target tensor is {} and {}'.format(max_length_inp,max_length_targ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaf9Mh4EejvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 50000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "03c49aed-0297-4ab1-9971-f34f846ae801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Creating training and validation sets using an 99-2 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.02)\n",
        "input_tensor_train,dev1_input_tensor_train,target_tensor_train,dev1_target_tensor_train=train_test_split(input_tensor_train,target_tensor_train,test_size=0.02)\n",
        "input_tensor_train,dev2_input_tensor_train,target_tensor_train,dev2_target_tensor_train=train_test_split(input_tensor_train,target_tensor_train,test_size=0.02)\n",
        "\n",
        "\n",
        "print('''Train set size {}, \n",
        "Test set size is {},\n",
        "Dev1 set size is {},        \n",
        "Dev2 set size is {}'''.format(len(input_tensor_train),len(input_tensor_val),len(dev1_input_tensor_train),len(dev2_input_tensor_train)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size 47059, \n",
            "Test set size is 1000,\n",
            "Dev1 set size is 980,        \n",
            "Dev2 set size is 961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "# def convert(lang, tensor):\n",
        "#   for t in tensor:\n",
        "#     if t!=0:\n",
        "#       print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "colab": {}
      },
      "source": [
        "# print (\"Input Language; index to word mapping\")\n",
        "# convert(inp_lang, input_tensor_train[0])\n",
        "# print ()\n",
        "# print (\"Target Language; index to word mapping\")\n",
        "# convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 256\n",
        "#steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "#vocab_inp_size = len(inp_lang.word_index)+1\n",
        "#vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJoapIYXlr7n",
        "colab_type": "code",
        "outputId": "7181e15c-5ac4-4164-dd9a-072088a763a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocab_inp_size,vocab_tar_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12999 6817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "outputId": "f5601335-6c05-458c-f19b-90c8c20fcd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(256), Dimension(16)]),\n",
              " TensorShape([Dimension(256), Dimension(12)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder and decoder model\n",
        "\n",
        "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://www.tensorflow.org/tutorials/seq2seq). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU\n",
        "\n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Ka8shFSZEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True,\n",
        "                               return_state=True,\n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1kMDbBlVZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru_fw = gru(self.enc_units)\n",
        "    self.bi_gru=tf.keras.layers.Bidirectional(self.gru_fw,merge_mode='ave')\n",
        "\n",
        "  def call(self, x, hidden1,hidden2):\n",
        "    x = self.embedding(x)\n",
        "    output, fw_state, bk_state = self.bi_gru(x, initial_state = [hidden1,hidden2])\n",
        "    state = tf.math.add(fw_state,bk_state)/2    \n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATMYNkdklxqZ",
        "colab_type": "code",
        "outputId": "d6882bf2-7738-4ab7-fcec-7a268de01928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden1 = encoder.initialize_hidden_state()\n",
        "sample_hidden2 = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden1,sample_hidden2)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "outputId": "f5f1c56c-bf7e-45c2-df8c-a4c5679c5c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (256, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (256, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = gru(self.dec_units)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "80a59489-e9cb-4397-fa76-79fe22ada60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, state,attention_weights = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (256, 6817)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G6oOKZv15XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='validation_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcgClMOdrYqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwel0v3L5DjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence,unk_id,sample_size):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence,language='english')\n",
        "    inputs = [inp_lang.word_index.get(i,unk_id) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "\n",
        "    hidden1 = tf.zeros((1, units))\n",
        "    hidden2 = tf.zeros((1, units))\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden1,hidden2)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        #predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        #Implement top-n sampling decoding technique\n",
        "        distribution = tf.argsort(predictions[0],direction='DESCENDING').numpy()[:sample_size]\n",
        "        #random.seed(2)\n",
        "        predicted_id = random.choice(distribution)\n",
        "        \n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "# @tf.function\n",
        "# def train_step(inp, targ, enc_hidden_fw,enc_hidden_bk):\n",
        "#   loss = 0\n",
        "#   acc = 0\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     enc_output, enc_hidden = encoder(inp, enc_hidden_fw,enc_hidden_bk)\n",
        "\n",
        "#     dec_hidden = enc_hidden\n",
        "\n",
        "#     dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "#     # Teacher forcing - feeding the target as the next input\n",
        "#     for t in range(1, targ.shape[1]):\n",
        "#       # passing enc_output to the decoder\n",
        "      \n",
        "#       predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "#       loss += loss_function(targ[:, t], predictions)\n",
        "#       acc+=train_accuracy(targ[:, t], predictions)\n",
        "\n",
        "\n",
        "#       # using teacher forcing\n",
        "#       dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "      \n",
        "  \n",
        "#   batch_loss = (loss / int(targ.shape[1]))\n",
        "#   batch_accuracy = (acc / int(targ.shape[1]))\n",
        "  \n",
        "#   variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  \n",
        "#   gradients = tape.gradient(loss, variables)\n",
        "  \n",
        "#   optimizer.apply_gradients(zip(gradients, variables))\n",
        "  \n",
        "# #   train_loss(loss)\n",
        "# #   train_accuracy(targ[:, t], predictions)\n",
        "  \n",
        "#   return (batch_loss,batch_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxA5e_aa9vgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_translation_with_greedy(encoder,decoder):\n",
        "  \n",
        "  for val_ip,target in zip(dev1_input_tensor_train,dev1_target_tensor_train):\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([val_ip],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    hidden1 = tf.zeros((1, units))\n",
        "    hidden2 = tf.zeros((1, units))\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden1,hidden2)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "    \n",
        "    result=[]\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result.append(predicted_id)\n",
        "      \n",
        "        if predicted_id == '1':\n",
        "            target = [tf.expand_dims(i, 0) for i in target]\n",
        "            result = [tf.expand_dims(i, 0) for i in result]\n",
        "            \n",
        "            return valid_accuracy(target,result).numpy()/len(target)\n",
        "            \n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    target = [tf.expand_dims(i, 0) for i in target]\n",
        "    result = [tf.expand_dims(i, 0) for i in result]\n",
        "    return valid_accuracy(target,result).numpy()/len(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HqMtU-lFcTMw",
        "outputId": "e4f3d40e-6b15-4365-c9e6-fa861d01f247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        acc=0\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden,hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                acc+=train_accuracy(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        batch_accuracy = (acc / int(targ.shape[1]))\n",
        "        total_loss += batch_loss\n",
        "        total_acc  += batch_accuracy\n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Train_accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy(),batch_accuracy/BATCH_SIZE))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    #if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f} Train_accuracy {:.4f}'.format(epoch + 1,total_loss/N_BATCH,total_acc / (N_BATCH*BATCH_SIZE)))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "      checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "      dev_Accuracy = batch_translation_with_greedy(encoder,decoder)\n",
        "      print('dev set accuracy on the entire dev set 1 is {}'.format(dev_Accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0703 05:46:17.115728 139874475698048 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:454: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.7729 Train_accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 2.2323 Train_accuracy 0.0006\n",
            "Epoch 1 Loss 434.9617 Train_accuracy 0.0005\n",
            "Time taken for 1 epoch 126.1062536239624 sec\n",
            "\n",
            "dev set accuracy on the entire dev set 1 is 0.027777778605620067\n",
            "Epoch 2 Batch 0 Loss 1.8941 Train_accuracy 0.0007\n",
            "Epoch 2 Batch 100 Loss 1.7521 Train_accuracy 0.0008\n",
            "Epoch 2 Loss 326.2333 Train_accuracy 0.0007\n",
            "Time taken for 1 epoch 126.77864170074463 sec\n",
            "\n",
            "dev set accuracy on the entire dev set 1 is 0.027777778605620067\n",
            "Epoch 3 Batch 0 Loss 1.5481 Train_accuracy 0.0008\n",
            "Epoch 3 Batch 100 Loss 1.5056 Train_accuracy 0.0009\n",
            "Epoch 3 Loss 276.9001 Train_accuracy 0.0009\n",
            "Time taken for 1 epoch 126.92377185821533 sec\n",
            "\n",
            "dev set accuracy on the entire dev set 1 is 0.027777778605620067\n",
            "Epoch 4 Batch 0 Loss 1.5481 Train_accuracy 0.0009\n",
            "Epoch 4 Batch 100 Loss 1.5056 Train_accuracy 0.0009\n",
            "Epoch 4 Loss 276.9002 Train_accuracy 0.0009\n",
            "Time taken for 1 epoch 128.12685680389404 sec\n",
            "\n",
            "dev set accuracy on the entire dev set 1 is 0.027777778605620067\n",
            "Epoch 5 Batch 0 Loss 1.3524 Train_accuracy 0.0010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "# def evaluate_using_bs(sentence,unk_id,beam_width):\n",
        "#     units = 1024\n",
        "#     beam_width = beam_width\n",
        "#     result = ''\n",
        "#     hidden1 = tf.zeros((1, units))\n",
        "#     hidden2 = tf.zeros((1, units))\n",
        "#     embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)\n",
        "    \n",
        "#     end_token = 2\n",
        "    \n",
        "#     beam_cel = tf.nn.rnn_cell.GRUCell(units)\n",
        "#     sentence = preprocess_sentence(sentence,language='english')\n",
        "#     inputs = [inp_lang.word_index.get(i,unk_id) for i in sentence.split(' ')]\n",
        "#     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "#                                                            maxlen=max_length_inp,\n",
        "#                                                            padding='post')\n",
        "#     inputs              = tf.convert_to_tensor(inputs)\n",
        "#     enc_out, enc_hidden = encoder(inputs, hidden1,hidden2)\n",
        "#     batch_size_beam      = tf.shape(enc_out)[0]\n",
        "#     start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [ batch_size_beam//beam_width ] )\n",
        "#     #print('till1')\n",
        "#     enc_rnn_out_beam   = tf.contrib.seq2seq.tile_batch(enc_out, beam_width )\n",
        "#     #print('till2')\n",
        "#     #seq_len_beam       = tf.contrib.seq2seq.tile_batch(enc_out.shape[1], beam_width )\n",
        "#     #print('till3')\n",
        "#     enc_rnn_state_beam = tf.contrib.seq2seq.tile_batch(enc_hidden, beam_width )\n",
        "#     #print('till3')\n",
        "#     out_layer          = tf.keras.layers.Dense(vocab_tar_size)\n",
        "#     #print('out')\n",
        "#     attn_mech_beam = tf.contrib.seq2seq.BahdanauAttention(num_units = units,  memory = enc_out, normalize=True)\n",
        "#     print('cell_imp1')\n",
        "#     cell_beam = tf.contrib.seq2seq.AttentionWrapper(cell=beam_cel,attention_mechanism=attn_mech_beam,\n",
        "#                                                     attention_layer_size=units)\n",
        "#     print('cell_imp2')\n",
        "    \n",
        "#     #initial_state_beam = cell_beam.zero_state(batch_size=batch_size_beam,dtype=tf.float32).clone(cell_state=enc_rnn_state_beam)\n",
        "#     initial_state_beam = cell_beam.zero_state(batch_size_beam, tf.float32)\n",
        "#     print('cell_imp3')   \n",
        "#     print(type(cell_beam),'cb')\n",
        "#     #print((embedding.shape),'eb_out')\n",
        "#     #print(tf.shape(cell_beam))\n",
        "#     my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell = cell_beam,\n",
        "#                                                      embedding = embedding,\n",
        "#                                                      start_tokens = start_tokens,\n",
        "#                                                      end_token = end_token,\n",
        "#                                                      initial_state = initial_state_beam,\n",
        "#                                                      beam_width = beam_width,\n",
        "#                                                      output_layer=out_layer)\n",
        "#     print('hrere')\n",
        "#     beam_output, t1 , t2 = tf.contrib.seq2seq.dynamic_decode(my_decoder)\n",
        "\n",
        "#     beam_logits = tf.no_op()\n",
        "#     predicted_ids = beam_output.predicted_ids\n",
        "\n",
        "#     for predicted_id in predicted_ids:\n",
        "#       result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "#       if targ_lang.index_word[predicted_id] == '<end>':\n",
        "#           return result, sentence\n",
        "\n",
        "        \n",
        "\n",
        "#     return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgjRD2CzksNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "\tsequences = [[list(), 1.0]]\n",
        "\t# walk over each step in sequence\n",
        "\tfor row in data:\n",
        "\t\tall_candidates = list()\n",
        "\t\t# expand each current candidate\n",
        "\t\tfor i in range(len(sequences)):\n",
        "\t\t\tseq, score = sequences[i]\n",
        "\t\t\tfor j in range(len(row)):\n",
        "\t\t\t\tcandidate = [seq + [j], score * -log(row[j])]\n",
        "\t\t\t\tall_candidates.append(candidate)\n",
        "\t\t# order all candidates by score\n",
        "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\t\t# select k best\n",
        "\t\tsequences = ordered[:k]\n",
        "\treturn sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZSZZDU5iZEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_using_bs1(sentence,unk_id,beam_width):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence,language='english')\n",
        "    inputs = [inp_lang.word_index.get(i,unk_id) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "\n",
        "    hidden1 = tf.zeros((1, units))\n",
        "    hidden2 = tf.zeros((1, units))\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden1,hidden2)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "    data=[]\n",
        "    main=[]\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        data.append(tf.nn.softmax(predictions[0]).numpy())\n",
        "        \n",
        "        \n",
        "        bresult = beam_search_decoder(np.array(data), beam_width)\n",
        "        #print(bresult)\n",
        "        main.append(bresult[0][0])\n",
        "        for seq in bresult:\n",
        "          for predicted_id in seq[0]:\n",
        "            result += targ_lang.index_word[predicted_id] + ' '\n",
        "            if targ_lang.index_word[predicted_id] == '<end>':\n",
        "              return main, sentence\n",
        "            dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return main, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YISpPBBnNf9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_using_bs(sentence,beam_width):\n",
        "    result, sentence = evaluate_using_bs1(sentence,unk_id,beam_width)\n",
        "    #print(result[-1])\n",
        "    result = ' '.join([targ_lang.index_word[i] for i in result[-1]])\n",
        "    return result\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "men1WTbKsjmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#[targ_lang.index_word[i] for i in [9, 394, 9027, 9, 9028, 1099, 2]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbVt8xhGNMrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#translate_using_bs1('you work well only with log sentences',2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t0JCleGjg5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence,unk_id,sample_size):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence,language='english')\n",
        "    inputs = [inp_lang.word_index.get(i,unk_id) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "\n",
        "    hidden1 = tf.zeros((1, units))\n",
        "    hidden2 = tf.zeros((1, units))\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden1,hidden2)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        #predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        #Implement top-n sampling decoding technique\n",
        "        distribution = tf.argsort(predictions[0],direction='DESCENDING').numpy()[:sample_size]\n",
        "        #random.seed(2)\n",
        "        predicted_id = random.choice(distribution)\n",
        "        \n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence,sample_size):\n",
        "    result, sentence, attention_plot = evaluate(sentence,unk_id,sample_size=sample_size)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9x5mERjji3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {}
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).assert_consumed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DUQVLVqUE1YW",
        "colab": {}
      },
      "source": [
        "\n",
        "def calc_BLEU(decoder_type='greedy'):\n",
        "  start=time.time()\n",
        "  score = 0\n",
        "  score_2=0\n",
        "  score_3=0\n",
        "  score_4=0\n",
        "  for targ,sour_input in zip(targ_lang.sequences_to_texts(target_tensor_val),inp_lang.sequences_to_texts(input_tensor_val)):\n",
        "  \n",
        "      \n",
        "    sour_input = sour_input.replace('<start>','').replace('<end>','')\n",
        "    targ = targ.replace('<start>','').replace('<end>','')\n",
        "    \n",
        "    if decoder_type == 'greedy':\n",
        "      try:\n",
        "        result,_,_=evaluate(sour_input,unk_id,sample_size=1)\n",
        "      except KeyError:\n",
        "        continue\n",
        "        print(sour_input)\n",
        "        \n",
        "      score +=sentence_bleu([targ],result)\n",
        "      \n",
        "    elif decoder_type == 'sampling_dec':\n",
        "      try:\n",
        "        result_2,_,_=evaluate(sour_input,unk_id,sample_size=2)\n",
        "        result_3,_,_=evaluate(sour_input,unk_id,sample_size=3)\n",
        "        result_4,_,_=evaluate(sour_input,unk_id,sample_size=4)\n",
        "      except KeyError:\n",
        "        continue\n",
        "        print(sour_input)\n",
        "      score_2 +=sentence_bleu([targ],result_2)\n",
        "      score_3 +=sentence_bleu([targ],result_3)\n",
        "      score_4 +=sentence_bleu([targ],result_4)\n",
        "      score = (score_2+score_3+score_4)/3\n",
        "    \n",
        "    elif decoder_type == 'beam_search':\n",
        "      try:\n",
        "        result_2=translate_using_bs(sour_input,beam_width=2)\n",
        "        result_3=translate_using_bs(sour_input,beam_width=3)\n",
        "        result_4=translate_using_bs(sour_input,beam_width=4)\n",
        "      except KeyError:\n",
        "        continue\n",
        "        print(sour_input)\n",
        "      score_2 +=sentence_bleu([targ],result_2)\n",
        "      score_3 +=sentence_bleu([targ],result_3)\n",
        "      score_4 +=sentence_bleu([targ],result_4)\n",
        "      score = (score_2+score_3+score_4)/3\n",
        "  print(score)        \n",
        "  print('processing time for {0} decoder is {1}'.format(decoder_type,str(time.time()-start)))\n",
        "  return(score/len(target_tensor_val) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOOBh09GDMKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Bleu_score_beam_search = calc_BLEU('beam_search')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r4gDnhDUM6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Bleu_score_greedy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rXi3HVnm223",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Bleu_score_greedy = calc_BLEU('greedy')\n",
        "print(Bleu_score_greedy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_05YjxhqK5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(2)\n",
        "n=1\n",
        "translate(u'you work well only with need long sentence',n)\n",
        "translate(u'do not take sufficient water',n)\n",
        "translate(u'You must specify a folder',n)\n",
        "translate(u'Executor of His own will',n)\n",
        "translate(u'This will keep getting better',n)\n",
        "translate(u'Sorry for confusing you',n)\n",
        "translate(u'Hey who are you man',n)\n",
        "translate(u'Message from server',n)\n",
        "translate(u\"I have three sons\",n)\n",
        "translate(u'it should be done',n)\n",
        "translate(u'Are you sure you want to delete this feed?',n)\n",
        "translate(u'there is a mark on the ruler',n)\n",
        "translate(u'Shooting in America',n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnOX8gUJUbhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_beams=[2,3,4]\n",
        "for i in n_beams:\n",
        "  n=i\n",
        "  translate_using_bs('you work well only with need long sentence',n)\n",
        "  translate_using_bs('do not take sufficient water',n)\n",
        "  translate_using_bs('You must specify a folder',n)\n",
        "  translate_using_bs(u'Executor of His own will',n)\n",
        "  translate_using_bs(u'This will keep getting better',n)\n",
        "  translate_using_bs(u'Sorry for confusing you',n)\n",
        "  translate_using_bs(u'Hey who are you man',n)\n",
        "  translate_using_bs(u'Message from server',n)\n",
        "  translate_using_bs(\"I have three sons\",n)\n",
        "  translate_using_bs('it should be done',n)\n",
        "  translate_using_bs('Are you sure you want to delete this feed ?',n)\n",
        "  translate_using_bs('there is a mark on the ruler',n)\n",
        "  translate_using_bs('Shooting in America',n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjc-sTKhPzvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Bleu_score_beam_search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2naeWehygzFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Baseline BLEU = 0.21320239655511655\n",
        "Bleu_score_beam_search = calc_BLEU('beam_search')\n",
        "Bleu_score_greedy = calc_BLEU('greedy')\n",
        "Bleu_score_sampling_decoder = calc_BLEU('sampling_dec')\n",
        "print(Bleu_score_beam_search)\n",
        "print(Bleu_score_greedy)\n",
        "print(Bleu_score_sampling_decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9nhGzD8P05i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,15), [6.56,5.6,5.44,5.35,4.58,3.92,3.4,2.92,2.4,2.18,1.9,1.65,1.43,1.28], 'ro')\n",
        "#plt.axis([0, 6, 0, 20])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8wCLyBQwk9",
        "colab_type": "text"
      },
      "source": [
        " ## Read this ...very important\n",
        " \n",
        " https://github.com/tensorflow/nmt#beam-search\n",
        " \n",
        " ## Word embedding reference\n",
        " \n",
        " https://github.com/monk1337/word_embedding-in-tensorflow/blob/master/Use%20Pre-trained%20word_embedding%20in%20Tensorflow.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "* Experiment with training on a larger dataset, or using more epochs\n"
      ]
    }
  ]
}