{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "En2Ta_transformer_beam_search.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenjune17/Neural-Machine-Translation-English-Tamil-model/blob/master/En2Ta_transformer_beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtB32GsJVqm7",
        "colab_type": "text"
      },
      "source": [
        "## Features \n",
        "\n",
        "Not have experience Out off vocab issue still since it uses subword encoding tech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxeD4fZbBDSj",
        "colab_type": "text"
      },
      "source": [
        "## Read transformer\n",
        "## Read BERT\n",
        "## Run the prediction task for BERT seperately\n",
        "## Train with the model with the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0s2Mo-tLKA",
        "colab_type": "code",
        "outputId": "7162b78d-c8e3-4dd5-edfa-89d062f0ce75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "#!pip install tf-nightly-gpu\n",
        "!pip install tensorflow-gpu==2.0.0-beta1 \n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "if not os.path.exists('drive'):\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 102kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDEq3-A8qyEh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Patch to add eng_tam_parallel text to tensorflow datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngGlE3e1ehcc",
        "colab_type": "code",
        "outputId": "456f0d3c-e500-404a-aaf0-e0767b467322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "from math import log\n",
        "\n",
        "if not os.path.exists('datasets'):\n",
        "  !git clone https://github.com/tensorflow/datasets\n",
        "if not os.path.exists('Neural-Machine-Translation-English-Tamil-model'):\n",
        "  !git clone https://github.com/praveenjune17/Neural-Machine-Translation-English-Tamil-model\n",
        "  !unzip Neural-Machine-Translation-English-Tamil-model/Transformer_en_tam.zip\n",
        "\n",
        "!python datasets/tensorflow_datasets/scripts/create_new_dataset.py \\\n",
        "  --dataset en_tam_parallel_text \\\n",
        "  --type translate"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'datasets'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 12409 (delta 92), reused 103 (delta 14), pack-reused 12219\u001b[K\n",
            "Receiving objects: 100% (12409/12409), 84.19 MiB | 31.45 MiB/s, done.\n",
            "Resolving deltas: 100% (8766/8766), done.\n",
            "Cloning into 'Neural-Machine-Translation-English-Tamil-model'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 266 (delta 24), reused 0 (delta 0), pack-reused 215\u001b[K\n",
            "Receiving objects: 100% (266/266), 122.99 MiB | 33.65 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "Archive:  Neural-Machine-Translation-English-Tamil-model/Transformer_en_tam.zip\n",
            "  inflating: en_tam_parallel_text.py  \n",
            "  inflating: en_tam_parallel_text.txt  \n",
            "  inflating: en_tam_parallel_text_test.py  \n",
            "Dataset generated in /usr/local/lib/python3.6/dist-packages/tensorflow_datasets\n",
            "You can start with searching TODO(en_tam_parallel_text).\n",
            "Please check this `https://github.com/tensorflow/datasets/blob/master/docs/add_dataset.md`for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "#Copy the patch and then import tfds\n",
        "path = '/usr/local/lib/python3.6/dist-packages/tensorflow_datasets'\n",
        "shutil.copy('../content/en_tam_parallel_text.py', os.path.join(path, 'translate/en_tam_parallel_text.py'))\n",
        "shutil.copy('../content/en_tam_parallel_text_test.py', os.path.join(path, 'translate/en_tam_parallel_text_test.py'))\n",
        "shutil.copy('../content/en_tam_parallel_text.txt', os.path.join(path, 'url_checksums/en_tam_parallel_text.txt'))\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RCEKotqosGfq"
      },
      "source": [
        "Create a custom subwords tokenizer from the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P82Z5QoEhV2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# en_tam_ds = defaultdict(list)\n",
        "# #List of available datasets in the package\n",
        "# names = ['GNOME_v1_en_to_ta', 'GNOME_v1_en_AU_to_ta', 'GNOME_v1_en_CA_to_ta', \n",
        "#          'GNOME_v1_en_GB_to_ta', 'GNOME_v1_en_US_to_ta', 'KDE4_v2_en_to_ta', \n",
        "#          'KDE4_v2_en_GB_to_ta', 'Tatoeba_v20190709_en_to_ta', 'Ubuntu_v14.10_en_to_ta_LK', \n",
        "#          'Ubuntu_v14.10_en_GB_to_ta_LK', 'Ubuntu_v14.10_en_AU_to_ta_LK', 'Ubuntu_v14.10_en_CA_to_ta_LK', \n",
        "#          'Ubuntu_v14.10_en_US_to_ta_LK', 'Ubuntu_v14.10_en_to_ta', 'Ubuntu_v14.10_en_GB_to_ta', \n",
        "#          'Ubuntu_v14.10_en_AU_to_ta', 'Ubuntu_v14.10_en_CA_to_ta', 'Ubuntu_v14.10_en_NZ_to_ta', \n",
        "#          'Ubuntu_v14.10_en_US_to_ta', 'OpenSubtitles_v2018_en_to_ta', 'OpenSubtitles_v2016_en_to_ta',\n",
        "#          'en_ta']\n",
        "\n",
        "# for name in names:\n",
        "#   en_tam_ds[(name,'metadata_'+name)] = tfds.load('en_tam_parallel_text/'+name, \n",
        "#                                            with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIZTBPd9YP9l",
        "colab_type": "text"
      },
      "source": [
        "### Load the old vocab file from G-drive if available else create.\n",
        "\n",
        "Note :- default vocab size is assumed to be 2^13, so if want to change it then don't load the file from G-drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRVbDgQ3Vkj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 534778 #train data line count\n",
        "en_vocab_path = '../content/drive/My Drive/Neural_machine_translation/Transformer_model/vocab_en_'+str(count)\n",
        "ta_vocab_path = '../content/drive/My Drive/Neural_machine_translation/Transformer_model/vocab_ta_'+str(count)\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_path)\n",
        "tokenizer_ta = tfds.features.text.SubwordTextEncoder.load_from_file(ta_vocab_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1P46D6J0H58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #initialize the first dataset to the train_examples variable\n",
        "# #Concatenate all the train datasets\n",
        "# train_examples = en_tam_ds[('GNOME_v1_en_to_ta', 'metadata_GNOME_v1_en_to_ta')][0]['train']\n",
        "# for typ in list(en_tam_ds.keys())[1:]:\n",
        "#   train_examples = train_examples.concatenate(en_tam_ds[typ][0]['train'])\n",
        "#   #validation and test sets are only available for a single typ\n",
        "#   if typ[0] == 'en_ta':\n",
        "#     test_examples = en_tam_ds[typ][0]['test']\n",
        "#     validation_examples = en_tam_ds[typ][0]['validation']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIbi7BnPfylt",
        "colab_type": "text"
      },
      "source": [
        "## Goal\n",
        "\n",
        "\n",
        "a) Try to get a subset of the datset for checking the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bcRp7VcQ5m6g",
        "colab": {}
      },
      "source": [
        "#BUFFER_SIZE = 20000\n",
        "#BATCH_SIZE = 64 * 4\n",
        "MAX_LENGTH = 50                  #drop examples of len 40 tokens\n",
        "train_from_scratch = False\n",
        "if not train_from_scratch:\n",
        "  new_folder_name = '28julyaft'     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGi4PoVakxdc"
      },
      "source": [
        "Add a start and end token to the input and target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZwnPr4R055s",
        "colab": {}
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang1.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_ta.vocab_size] + tokenizer_ta.encode(\n",
        "      lang2.numpy()) + [tokenizer_ta.vocab_size+1]\n",
        "  #print(len(lang1),len(lang2))\n",
        "  \n",
        "  return lang1, lang2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c081xPGv1CPI",
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  \n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tx1sFbR-9fRs"
      },
      "source": [
        "Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mah1cS-P70Iz",
        "colab": {}
      },
      "source": [
        "def tf_encode(en, ta):\n",
        "  return tf.py_function(encode, [en, ta], [tf.int64, tf.int64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WhIOZjMNKujn",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Rz82wEs5biZ",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  sines = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  cosines = np.cos(angle_rads[:, 1::2])\n",
        "  \n",
        "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "  \n",
        "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2i8-e1s8ti9",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions so that we can add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dVxS8OPI9uI0",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vsxEE_-Wa1gF"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
        "\n",
        "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, so that we get a gentler softmax.\n",
        "\n",
        "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LazzUq3bJ5SH",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n90YjClyInFy",
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Multi-head attention consists of four parts:\n",
        "*    Linear layers and split into heads.\n",
        "*    Scaled dot-product attention.\n",
        "*    Concatenation of heads.\n",
        "*    Final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BSV3PPKsYecw",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ET7xLt0yCT6Z",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yScbC0MUH8dS"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfYJG-Kvgwy2"
      },
      "source": [
        "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
        "\n",
        "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
        "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ncyS-Ms3i2x_",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9SoX0-vd1hue",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jpEox7gJ8FCI",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d5_d5-PLQXwY",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
        "               rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PED3bIpOYkBu",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDoDtmA6vlTp",
        "colab_type": "code",
        "outputId": "53efbab3-8820-4225-a540-cd5bcfcd5835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size)\n",
        "start_token = [tokenizer_en.vocab_size]\n",
        "end_token = [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "# inp sentence is english, hence adding the start and end token\n",
        "inp_sentence = 'hi this is praveen'\n",
        "inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "decoder_input = [tokenizer_ta.vocab_size]\n",
        "output = tf.expand_dims(decoder_input, 0)\n",
        "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "      encoder_input, output)\n",
        "fn_out, _ = sample_transformer(encoder_input, \n",
        "                               output, \n",
        "                               training=False, \n",
        "                               enc_padding_mask=enc_padding_mask, \n",
        "                               look_ahead_mask=combined_mask,\n",
        "                               dec_padding_mask=dec_padding_mask)\n",
        "\n",
        "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "print(fn_out)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 8380)\n",
            "tf.Tensor(\n",
            "[[[ 0.00366749  0.18964183  0.27181947 ...  0.1326359  -0.42915836\n",
            "   -0.36908984]]], shape=(1, 1, 8380), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGpL0T4PcIOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3b8bbc75-b4df-489b-d6d4-68d7d0b4e282"
      },
      "source": [
        "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "      encoder_input, nop)\n",
        "fn_out, _ = sample_transformer(encoder_input, \n",
        "                               nop, \n",
        "                               training=False, \n",
        "                               enc_padding_mask=enc_padding_mask, \n",
        "                               look_ahead_mask=combined_mask,\n",
        "                               dec_padding_mask=dec_padding_mask)\n",
        "print(encoder_input)\n",
        "print(nop)\n",
        "print(fn_out)\n",
        "#output = tf.concat([output, [[123]]], axis=-1)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[8262 7289   18    9 1121  353  421 8263]], shape=(1, 8), dtype=int32)\n",
            "tf.Tensor([[8378  123  123  123  123 8378  133  173  193  123]], shape=(1, 10), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[[ 0.00366767  0.189642    0.27181953 ...  0.13263631 -0.42915773\n",
            "   -0.3690896 ]\n",
            "  [-0.06454872  0.04790066  0.4150741  ...  0.13959071 -0.44853297\n",
            "   -0.3398813 ]\n",
            "  [-0.09530322  0.0264617   0.43166703 ...  0.13068306 -0.40416303\n",
            "   -0.3058929 ]\n",
            "  ...\n",
            "  [-0.21828337  0.00857975  0.57766217 ...  0.02809261 -0.2824666\n",
            "   -0.40832213]\n",
            "  [-0.04752382  0.1327323   0.37792093 ...  0.02372044 -0.3367924\n",
            "   -0.26509893]\n",
            "  [-0.06972325  0.05435599  0.4849574  ...  0.03791654 -0.35244283\n",
            "   -0.32217568]]], shape=(1, 10, 8380), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1nlsH6ejBni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6e0753b1-444d-4eff-e601-a2edf9d216ae"
      },
      "source": [
        "print(encoder_input)\n",
        "print(nop[:,-1:])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[8262 7289   18    9 1121  353  421 8263]], shape=(1, 8), dtype=int32)\n",
            "tf.Tensor([[123]], shape=(1, 1), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0tmuRKcbrJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "already incoming id :- select last one \n",
        "  concat that with the exsisting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1niz27ciidc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb45e3ab-a3c2-4a36-e0fb-ee1f691bc2d0"
      },
      "source": [
        "boutput = tf.convert_to_tensor([[8378,  133,  173,  193,  123]])\n",
        "boutput"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=118187, shape=(1, 5), dtype=int32, numpy=array([[8378,  133,  173,  193,  123]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9zLwo4Kiu4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nop = tf.concat([output, boutput], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9upEQ3IlcBhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff143deb-46c0-4262-c584-f74776e85e4c"
      },
      "source": [
        "create a second row for the output and see whether it works "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=118183, shape=(1, 5), dtype=int32, numpy=array([[8378,  123,  123,  123,  123]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5a-EFJ_X6Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_token = [tokenizer_en.vocab_size]\n",
        "end_token = [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "# inp sentence is english, hence adding the start and end token\n",
        "inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "# as the target is tamil, the first word to the transformer should be the\n",
        "# english start token.\n",
        "decoder_input = [tokenizer_ta.vocab_size]\n",
        "output = tf.expand_dims(decoder_input, 0)\n",
        "print(output.shape)\n",
        "for i in range(MAX_LENGTH):\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "      encoder_input, output)\n",
        "\n",
        "  # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "  predictions, attention_weights = transformer(encoder_input, \n",
        "                                               output,\n",
        "                                               False,\n",
        "                                               enc_padding_mask,\n",
        "                                               combined_mask,\n",
        "                                               dec_padding_mask)\n",
        "\n",
        "  # select the last word from the seq_len dimension\n",
        "  predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "  predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "  # return the result if the predicted_id is equal to the end token\n",
        "  if tf.equal(predicted_id, tokenizer_ta.vocab_size+1):\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "# concatentate the predicted_id to the output which is given to the decoder\n",
        "# as its input.\n",
        "  output = tf.concat([output, predicted_id], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "outputId": "d693a8d2-6ee5-4701-bfec-9b5a96a5c1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_layers = 4  #denoted as 'L' in BERT , no.of blocks\n",
        "d_model = 256   #ll sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512. in bert\n",
        "dff = 1024      #denoted as 'H' in BERT \n",
        "num_heads = 4   #denoted as 'A' in BERT\n",
        "\n",
        "\n",
        "'''\n",
        "BERT config\n",
        "In all cases we set the feed-forward/filter size to be 4H,\n",
        "i.e., 3072 for the H = 768 and 4096 for the H = 1024.\n",
        "'''\n",
        "input_vocab_size = tokenizer_en.vocab_size + 2\n",
        "target_vocab_size = tokenizer_ta.vocab_size + 2\n",
        "dropout_rate = 0.3\n",
        "\n",
        "print('english vocab size is {} '.format(input_vocab_size))\n",
        "print('tamil vocab size is {}'.format(target_vocab_size))\n",
        "\n",
        "\n",
        "##Hyper parameter used in the First run \n",
        "# num_layers = 4\n",
        "# d_model = 256\n",
        "# dff = 1024  \n",
        "# num_heads = 4\n",
        "\n",
        "# input_vocab_size = tokenizer_ta.vocab_size + 2\n",
        "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "# dropout_rate = 0.3\n",
        "#'''\n",
        "if not train_from_scratch:\n",
        "  model_name = 'model_'+'_'.join((str(num_layers), str(d_model), str(dff), str(num_heads), str(input_vocab_size), str(target_vocab_size)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english vocab size is 8264 \n",
            "tamil vocab size is 8380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iYQdOO1axwEI",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f33ZCgvHpPdG",
        "outputId": "7b35c1af-24bd-4411-9b5a-74cbc42ba44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmIa/6b5CMgEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MlhsJMm0TW_B",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67oqVHiT0Eiu",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phlyxMnm-Tpx",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UiysUa--4tOU",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZOJUSB1T8GjM",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCwJnBBDkAeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_path = '../content/drive/My Drive/Neural_machine_translation/Transformer_model/checkpoints/'\\\n",
        "              +new_folder_name\n",
        "\n",
        "if train_from_scratch == True:\n",
        "  shutil.rmtree('../content/checkpoints')\n",
        "  checkpoint_path = './checkpoints/train'\n",
        "else:\n",
        "  checkpoint_path = new_path +'/'+model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "outputId": "872e4d0d-e57d-40e2-cde2-4e18d9b77e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if (ckpt_manager.latest_checkpoint):\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
        "  print ('Latest checkpoint restored is {} !!'.format(ckpt_manager.latest_checkpoint.split('/')[-1]))\n",
        "  print ('size of the checkpoint directory is {}MB '.format(sum(os.path.getsize(os.path.join(checkpoint_path,f)) for f in os.listdir(checkpoint_path))/(1024*1024)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored is ckpt-148 !!\n",
            "size of the checkpoint directory is 789.6208267211914MB \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Di_Yaa1gf9r"
      },
      "source": [
        "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
        "\n",
        "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
        "\n",
        "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y6APsFrgImLW"
      },
      "source": [
        "The following steps are used for evaluation:\n",
        "\n",
        "* Encode the input sentence using the tamil3 tokenizer (`tokenizer_ta`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* Select the last word and calculate the argmax of that.\n",
        "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
        "\n",
        "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U7DA5rucWbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#visualize train loss and Validation loss in tensorboard (@batch % 50 == 0)\n",
        "#Follow karpthy's suggestion\n",
        "#Small transformer model and feed it with a small data and increase the complexity untill the loss reaches zero\n",
        "#Shuffle the 500K records and select 50K for training and select the best model (dataset.take(count))\n",
        "#BLEU score code as evaluation metric for validation set\n",
        "#beam_search decoder (paper highlights)\n",
        "#use BERT to intialize weights (paper highlights) and train using TPU\n",
        "#Publish paper , go through the IIT paper and get the results of validation and test\n",
        "#Transformers with beamsearch\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQEstVGg0WH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_en.vocab_size]\n",
        "  end_token = [tokenizer_en.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is english, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is tamil, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_ta.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "  print(output.shape)\n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, tokenizer_ta.vocab_size+1):\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRzUL1tmGeb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_ta.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_en.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([tokenizer_ta.decode([i]) for i in result \n",
        "                        if i < tokenizer_ta.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u0qZidEGjIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  \n",
        "  predicted_sentence = tokenizer_ta.decode([i for i in result \n",
        "                                            if i < tokenizer_ta.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3z_UCHPlKsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Tensor2Tensor Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Implementation of beam search with penalties.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from tensor2tensor.layers import common_layers\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.ops import inplace_ops\n",
        "from tensorflow.python.util import nest\n",
        "\n",
        "# Assuming EOS_ID is 1\n",
        "EOS_ID = 1\n",
        "# Default value for INF\n",
        "INF = 1. * 1e7\n",
        "\n",
        "\n",
        "def _merge_beam_dim(tensor):\n",
        "  \"\"\"Reshapes first two dimensions in to single dimension.\n",
        "\n",
        "  Args:\n",
        "    tensor: Tensor to reshape of shape [A, B, ...]\n",
        "\n",
        "  Returns:\n",
        "    Reshaped tensor of shape [A*B, ...]\n",
        "  \"\"\"\n",
        "  shape = common_layers.shape_list(tensor)\n",
        "  shape[0] *= shape[1]  # batch -> batch * beam_size\n",
        "  shape.pop(1)  # Remove beam dim\n",
        "  return tf.reshape(tensor, shape)\n",
        "\n",
        "\n",
        "def _unmerge_beam_dim(tensor, batch_size, beam_size):\n",
        "  \"\"\"Reshapes first dimension back to [batch_size, beam_size].\n",
        "\n",
        "  Args:\n",
        "    tensor: Tensor to reshape of shape [batch_size*beam_size, ...]\n",
        "    batch_size: Tensor, original batch size.\n",
        "    beam_size: int, original beam size.\n",
        "\n",
        "  Returns:\n",
        "    Reshaped tensor of shape [batch_size, beam_size, ...]\n",
        "  \"\"\"\n",
        "  shape = common_layers.shape_list(tensor)\n",
        "  new_shape = [batch_size] + [beam_size] + shape[1:]\n",
        "  return tf.reshape(tensor, new_shape)\n",
        "\n",
        "\n",
        "def _expand_to_beam_size(tensor, beam_size):\n",
        "  \"\"\"Tiles a given tensor by beam_size.\n",
        "\n",
        "  Args:\n",
        "    tensor: tensor to tile [batch_size, ...]\n",
        "    beam_size: How much to tile the tensor by.\n",
        "\n",
        "  Returns:\n",
        "    Tiled tensor [batch_size, beam_size, ...]\n",
        "  \"\"\"\n",
        "  tensor = tf.expand_dims(tensor, axis=1)\n",
        "  tile_dims = [1] * tensor.shape.ndims\n",
        "  tile_dims[1] = beam_size\n",
        "\n",
        "  return tf.tile(tensor, tile_dims)\n",
        "\n",
        "\n",
        "def get_state_shape_invariants(tensor):\n",
        "  \"\"\"Returns the shape of the tensor but sets middle dims to None.\"\"\"\n",
        "  shape = tensor.shape.as_list()\n",
        "  for i in range(1, len(shape) - 1):\n",
        "    shape[i] = None\n",
        "  return tf.TensorShape(shape)\n",
        "\n",
        "\n",
        "def compute_batch_indices(batch_size, beam_size):\n",
        "  \"\"\"Computes the i'th coordinate that contains the batch index for gathers.\n",
        "\n",
        "  Batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..]. It says which\n",
        "  batch the beam item is in. This will create the i of the i,j coordinate\n",
        "  needed for the gather.\n",
        "\n",
        "  Args:\n",
        "    batch_size: Batch size\n",
        "    beam_size: Size of the beam.\n",
        "  Returns:\n",
        "    batch_pos: [batch_size, beam_size] tensor of ids\n",
        "  \"\"\"\n",
        "  batch_pos = tf.range(batch_size * beam_size) // beam_size\n",
        "  batch_pos = tf.reshape(batch_pos, [batch_size, beam_size])\n",
        "  return batch_pos\n",
        "\n",
        "\n",
        "def fast_tpu_gather(params, indices, name=None):\n",
        "  \"\"\"Fast gather implementation for models running on TPU.\n",
        "\n",
        "  This function use one_hot and batch matmul to do gather, which is faster\n",
        "  than gather_nd on TPU. For params that have dtype of int32 (sequences to\n",
        "  gather from), batch_gather is used to keep accuracy.\n",
        "\n",
        "  Args:\n",
        "    params: A tensor from which to gather values.\n",
        "      [batch_size, original_size, ...]\n",
        "    indices: A tensor used as the index to gather values.\n",
        "      [batch_size, selected_size].\n",
        "    name: A string, name of the operation (optional).\n",
        "\n",
        "  Returns:\n",
        "    gather_result: A tensor that has the same rank as params.\n",
        "      [batch_size, selected_size, ...]\n",
        "  \"\"\"\n",
        "  with tf.compat.v1.name_scope(name):\n",
        "    dtype = params.dtype\n",
        "\n",
        "    def _gather(params, indices):\n",
        "      \"\"\"Fast gather using one_hot and batch matmul.\"\"\"\n",
        "      if dtype != tf.float32:\n",
        "        params = tf.cast(params, dtype=tf.float32)\n",
        "      shape = common_layers.shape_list(params)\n",
        "      indices_shape = common_layers.shape_list(indices)\n",
        "      ndims = params.shape.ndims\n",
        "      # Adjust the shape of params to match one-hot indices, which is the\n",
        "      # requirement of Batch MatMul.\n",
        "      if ndims == 2:\n",
        "        params = tf.expand_dims(params, axis=-1)\n",
        "      if ndims > 3:\n",
        "        params = tf.reshape(params, [shape[0], shape[1], -1])\n",
        "      gather_result = tf.matmul(\n",
        "          tf.one_hot(indices, shape[1], dtype=params.dtype), params)\n",
        "      if ndims == 2:\n",
        "        gather_result = tf.squeeze(gather_result, axis=-1)\n",
        "      if ndims > 3:\n",
        "        shape[1] = indices_shape[1]\n",
        "        gather_result = tf.reshape(gather_result, shape)\n",
        "      if dtype != tf.float32:\n",
        "        gather_result = tf.cast(gather_result, dtype)\n",
        "      return gather_result\n",
        "\n",
        "    # If the dtype is int32, use the gather instead of one_hot matmul to avoid\n",
        "    # precision loss. The max int value can be represented by bfloat16 in MXU is\n",
        "    # 256, which is smaller than the possible id values. Encoding/decoding can\n",
        "    # potentially used to make it work, but the benenfit is small right now.\n",
        "    if dtype == tf.int32:\n",
        "      gather_result = tf.compat.v1.batch_gather(params, indices)\n",
        "    else:\n",
        "      gather_result = _gather(params, indices)\n",
        "\n",
        "    return gather_result\n",
        "\n",
        "\n",
        "def _create_make_unique(inputs):\n",
        "  \"\"\"Replaces the lower bits of each element with iota.\n",
        "\n",
        "  The iota is used to derive the index, and also serves the purpose to\n",
        "  make each element unique to break ties.\n",
        "\n",
        "  Args:\n",
        "    inputs: A tensor with rank of 2 and dtype of tf.float32.\n",
        "      [batch_size, original_size].\n",
        "\n",
        "  Returns:\n",
        "    A tensor after element wise transformation, with dtype the same as inputs.\n",
        "    [batch_size, original_size].\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the rank of the input tensor does not equal 2.\n",
        "  \"\"\"\n",
        "  if inputs.shape.ndims != 2:\n",
        "    raise ValueError(\"Input of top_k_with_unique must be rank-2 \"\n",
        "                     \"but got: %s\" % inputs.shape)\n",
        "\n",
        "  height = inputs.shape[0]\n",
        "  width = inputs.shape[1]\n",
        "  zeros = tf.zeros([height, width], dtype=tf.int32)\n",
        "\n",
        "  # Count_mask is used to mask away the low order bits to ensure that every\n",
        "  # element is distinct.\n",
        "  log2_ceiling = int(math.ceil(math.log(int(width), 2)))\n",
        "  next_power_of_two = 1 << log2_ceiling\n",
        "  count_mask = ~(next_power_of_two - 1)\n",
        "  count_mask_r0 = tf.constant(count_mask)\n",
        "  count_mask_r2 = tf.fill([height, width], count_mask_r0)\n",
        "\n",
        "  # Smallest_normal is the bit representation of the smallest positive normal\n",
        "  # floating point number. The sign is zero, exponent is one, and the fraction\n",
        "  # is zero.\n",
        "  smallest_normal = 1 << 23\n",
        "  smallest_normal_r0 = tf.constant(smallest_normal, dtype=tf.int32)\n",
        "  smallest_normal_r2 = tf.fill([height, width], smallest_normal_r0)\n",
        "\n",
        "  # Low_bit_mask is used to mask away the sign bit when computing the absolute\n",
        "  # value.\n",
        "  low_bit_mask = ~(1 << 31)\n",
        "  low_bit_mask_r0 = tf.constant(low_bit_mask, dtype=tf.int32)\n",
        "  low_bit_mask_r2 = tf.fill([height, width], low_bit_mask_r0)\n",
        "\n",
        "  iota = tf.tile(tf.expand_dims(tf.range(width, dtype=tf.int32), 0),\n",
        "                 [height, 1])\n",
        "\n",
        "  # Compare the absolute value with positive zero to handle negative zero.\n",
        "  input_r2 = tf.bitcast(inputs, tf.int32)\n",
        "  abs_r2 = tf.bitwise.bitwise_and(input_r2, low_bit_mask_r2)\n",
        "  if_zero_r2 = tf.equal(abs_r2, zeros)\n",
        "  smallest_normal_preserving_sign_r2 = tf.bitwise.bitwise_or(\n",
        "      input_r2, smallest_normal_r2)\n",
        "  input_no_zeros_r2 = tf.compat.v1.where(\n",
        "      if_zero_r2, smallest_normal_preserving_sign_r2, input_r2)\n",
        "\n",
        "  # Discard the low-order bits and replace with iota.\n",
        "  and_r2 = tf.bitwise.bitwise_and(input_no_zeros_r2, count_mask_r2)\n",
        "  or_r2 = tf.bitwise.bitwise_or(and_r2, iota)\n",
        "  return tf.bitcast(or_r2, tf.float32)\n",
        "\n",
        "\n",
        "def _create_topk_unique(inputs, k):\n",
        "  \"\"\"Creates the top k values in sorted order with indices.\n",
        "\n",
        "  Args:\n",
        "    inputs: A tensor with rank of 2. [batch_size, original_size].\n",
        "    k: An integer, number of top elements to select.\n",
        "\n",
        "  Returns:\n",
        "    topk_r2: A tensor, the k largest elements. [batch_size, k].\n",
        "    topk_indices_r2: A tensor, indices of the top k values. [batch_size, k].\n",
        "  \"\"\"\n",
        "  height = inputs.shape[0]\n",
        "  width = inputs.shape[1]\n",
        "  neg_inf_r0 = tf.constant(-np.inf, dtype=tf.float32)\n",
        "  ones = tf.ones([height, width], dtype=tf.float32)\n",
        "  neg_inf_r2 = ones * neg_inf_r0\n",
        "  inputs = tf.compat.v1.where(tf.math.is_nan(inputs), neg_inf_r2, inputs)\n",
        "\n",
        "  # Select the current largest value k times and keep them in topk_r2. The\n",
        "  # selected largest values are marked as the smallest value to avoid being\n",
        "  # selected again.\n",
        "  tmp = inputs\n",
        "  topk_r2 = tf.zeros([height, k], dtype=tf.float32)\n",
        "  for i in range(k):\n",
        "    kth_order_statistic = tf.reduce_max(input_tensor=tmp, axis=1, keepdims=True)\n",
        "    k_mask = tf.tile(tf.expand_dims(tf.equal(tf.range(k), tf.fill([k], i)), 0),\n",
        "                     [height, 1])\n",
        "    topk_r2 = tf.compat.v1.where(k_mask, tf.tile(kth_order_statistic, [1, k]), topk_r2)\n",
        "    ge_r2 = tf.greater_equal(inputs, tf.tile(kth_order_statistic, [1, width]))\n",
        "    tmp = tf.compat.v1.where(ge_r2, neg_inf_r2, inputs)\n",
        "\n",
        "  log2_ceiling = int(math.ceil(math.log(float(int(width)), 2)))\n",
        "  next_power_of_two = 1 << log2_ceiling\n",
        "  count_mask = next_power_of_two - 1\n",
        "  mask_r0 = tf.constant(count_mask)\n",
        "  mask_r2 = tf.fill([height, k], mask_r0)\n",
        "  topk_r2_s32 = tf.bitcast(topk_r2, tf.int32)\n",
        "  topk_indices_r2 = tf.bitwise.bitwise_and(topk_r2_s32, mask_r2)\n",
        "  return topk_r2, topk_indices_r2\n",
        "\n",
        "\n",
        "def top_k_with_unique(inputs, k):\n",
        "  \"\"\"Finds the values and indices of the k largests entries.\n",
        "\n",
        "  Instead of doing sort like tf.nn.top_k, this function finds the max value\n",
        "  k times. The running time is proportional to k, which is be faster when k\n",
        "  is small. The current implementation supports only inputs of rank 2.\n",
        "  In addition, iota is used to replace the lower bits of each element, this\n",
        "  makes the selection more stable when there are equal elements. The\n",
        "  overhead is that output values are approximated.\n",
        "\n",
        "  Args:\n",
        "    inputs: A tensor with rank of 2. [batch_size, original_size].\n",
        "    k: An integer, number of top elements to select.\n",
        "\n",
        "  Returns:\n",
        "    top_values: A tensor, the k largest elements in sorted order.\n",
        "      [batch_size, k].\n",
        "    indices: A tensor, indices of the top_values. [batch_size, k].\n",
        "  \"\"\"\n",
        "  unique_inputs = _create_make_unique(tf.cast(inputs, tf.float32))\n",
        "  top_values, indices = _create_topk_unique(unique_inputs, k)\n",
        "  top_values = tf.cast(top_values, inputs.dtype)\n",
        "  return top_values, indices\n",
        "\n",
        "\n",
        "def compute_topk_scores_and_seq(sequences, scores, scores_to_gather, flags,\n",
        "                                beam_size, batch_size, prefix=\"default\",\n",
        "                                states_to_gather=None, use_tpu=False):\n",
        "  \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n",
        "\n",
        "  This function is used to grow alive, and finished. It takes sequences,\n",
        "  scores, and flags, and returns the top k from sequences, scores_to_gather,\n",
        "  and flags based on the values in scores.\n",
        "\n",
        "  This method permits easy introspection using tfdbg.  It adds three named ops\n",
        "  that are prefixed by `prefix`:\n",
        "    - _topk_seq: the tensor for topk_seq returned by this method.\n",
        "    - _topk_flags: the tensor for topk_finished_flags returned by this method.\n",
        "    - _topk_scores: the tensor for tokp_gathered_scores returned by this method.\n",
        "\n",
        "  Args:\n",
        "    sequences: Tensor of sequences that we need to gather from.\n",
        "      [batch_size, beam_size, seq_length]\n",
        "    scores: Tensor of scores for each sequence in sequences.\n",
        "      [batch_size, beam_size]. We will use these to compute the topk.\n",
        "    scores_to_gather: Tensor of scores for each sequence in sequences.\n",
        "      [batch_size, beam_size]. We will return the gathered scores from here.\n",
        "      Scores to gather is different from scores because for grow_alive, we will\n",
        "      need to return log_probs, while for grow_finished, we will need to return\n",
        "      the length penalized scores.\n",
        "    flags: Tensor of bools for sequences that say whether a sequence has reached\n",
        "      EOS or not\n",
        "    beam_size: int\n",
        "    batch_size: int\n",
        "    prefix: string that will prefix unique names for the ops run.\n",
        "    states_to_gather: dict (possibly nested) of decoding states.\n",
        "    use_tpu: A bool, whether to compute topk scores and sequences on TPU.\n",
        "\n",
        "  Returns:\n",
        "    Tuple of\n",
        "    (topk_seq [batch_size, beam_size, decode_length],\n",
        "     topk_gathered_scores [batch_size, beam_size],\n",
        "     topk_finished_flags[batch_size, beam_size])\n",
        "  \"\"\"\n",
        "  if not use_tpu:\n",
        "    _, topk_indexes = tf.nn.top_k(scores, k=beam_size)\n",
        "    # The next three steps are to create coordinates for tf.gather_nd to pull\n",
        "    # out the topk sequences from sequences based on scores.\n",
        "    # batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..]. It says which\n",
        "    # batch the beam item is in. This will create the i of the i,j coordinate\n",
        "    # needed for the gather\n",
        "    batch_pos = compute_batch_indices(batch_size, beam_size)\n",
        "\n",
        "    # top coordinates will give us the actual coordinates to do the gather.\n",
        "    # stacking will create a tensor of dimension batch * beam * 2, where the\n",
        "    # last dimension contains the i,j gathering coordinates.\n",
        "    top_coordinates = tf.stack([batch_pos, topk_indexes], axis=2)\n",
        "\n",
        "    # Gather up the highest scoring sequences.  For each operation added, give\n",
        "    # it a concrete name to simplify observing these operations with tfdbg.\n",
        "    # Clients can capture these tensors by watching these node names.\n",
        "    def gather(tensor, name):\n",
        "      return tf.gather_nd(tensor, top_coordinates, name=(prefix + name))\n",
        "    topk_seq = gather(sequences, \"_topk_seq\")\n",
        "    topk_flags = gather(flags, \"_topk_flags\")\n",
        "    topk_gathered_scores = gather(scores_to_gather, \"_topk_scores\")\n",
        "    if states_to_gather:\n",
        "      topk_gathered_states = nest.map_structure(\n",
        "          lambda state: gather(state, \"_topk_states\"), states_to_gather)\n",
        "    else:\n",
        "      topk_gathered_states = states_to_gather\n",
        "  else:\n",
        "    _, topk_indexes = top_k_with_unique(scores, k=beam_size)\n",
        "    # Gather up the highest scoring sequences.  For each operation added, give\n",
        "    # it a concrete name to simplify observing these operations with tfdbg.\n",
        "    # Clients can capture these tensors by watching these node names.\n",
        "    topk_seq = fast_tpu_gather(sequences, topk_indexes, prefix + \"_topk_seq\")\n",
        "    topk_flags = fast_tpu_gather(flags, topk_indexes, prefix + \"_topk_flags\")\n",
        "    topk_gathered_scores = fast_tpu_gather(scores_to_gather, topk_indexes,\n",
        "                                           prefix + \"_topk_scores\")\n",
        "    if states_to_gather:\n",
        "      topk_gathered_states = nest.map_structure(\n",
        "          # pylint: disable=g-long-lambda\n",
        "          lambda state: fast_tpu_gather(state, topk_indexes,\n",
        "                                        prefix + \"_topk_states\"),\n",
        "          states_to_gather)\n",
        "    else:\n",
        "      topk_gathered_states = states_to_gather\n",
        "  return topk_seq, topk_gathered_scores, topk_flags, topk_gathered_states\n",
        "\n",
        "\n",
        "def beam_search(symbols_to_logits_fn,\n",
        "                initial_ids,\n",
        "                beam_size,\n",
        "                decode_length,\n",
        "                vocab_size,\n",
        "                alpha,\n",
        "                states=None,\n",
        "                eos_id=EOS_ID,\n",
        "                stop_early=True,\n",
        "                use_tpu=False):\n",
        "  \"\"\"Beam search with length penalties.\n",
        "\n",
        "  Requires a function that can take the currently decoded symbols and return\n",
        "  the logits for the next symbol. The implementation is inspired by\n",
        "  https://arxiv.org/abs/1609.08144.\n",
        "\n",
        "  When running, the beam search steps can be visualized by using tfdbg to watch\n",
        "  the operations generating the output ids for each beam step.  These operations\n",
        "  have the pattern:\n",
        "    (alive|finished)_topk_(seq,scores)\n",
        "\n",
        "  Operations marked `alive` represent the new beam sequences that will be\n",
        "  processed in the next step.  Operations marked `finished` represent the\n",
        "  completed beam sequences, which may be padded with 0s if no beams finished.\n",
        "\n",
        "  Operations marked `seq` store the full beam sequence for the time step.\n",
        "  Operations marked `scores` store the sequence's final log scores.\n",
        "\n",
        "  The beam search steps will be processed sequentially in order, so when\n",
        "  capturing observed from these operations, tensors, clients can make\n",
        "  assumptions about which step is being recorded.\n",
        "\n",
        "  WARNING: Assumes 2nd dimension of tensors in `states` and not invariant, this\n",
        "  means that the shape of the 2nd dimension of these tensors will not be\n",
        "  available (i.e. set to None) inside symbols_to_logits_fn.\n",
        "\n",
        "  Args:\n",
        "    symbols_to_logits_fn: Interface to the model, to provide logits.\n",
        "        Shoud take [batch_size, decoded_ids] and return [batch_size, vocab_size]\n",
        "    initial_ids: Ids to start off the decoding, this will be the first thing\n",
        "        handed to symbols_to_logits_fn (after expanding to beam size)\n",
        "        [batch_size]\n",
        "    beam_size: Size of the beam.\n",
        "    decode_length: Number of steps to decode for.\n",
        "    vocab_size: Size of the vocab, must equal the size of the logits returned by\n",
        "        symbols_to_logits_fn\n",
        "    alpha: alpha for length penalty.\n",
        "    states: dict (possibly nested) of decoding states.\n",
        "    eos_id: ID for end of sentence.\n",
        "    stop_early: a boolean - stop once best sequence is provably determined.\n",
        "    use_tpu: A bool, whether to do beam search on TPU.\n",
        "\n",
        "  Returns:\n",
        "    Tuple of\n",
        "    (decoded beams [batch_size, beam_size, decode_length]\n",
        "     decoding probabilities [batch_size, beam_size])\n",
        "  \"\"\"\n",
        "  batch_size = common_layers.shape_list(initial_ids)[0]\n",
        "\n",
        "  # Assume initial_ids are prob 1.0\n",
        "  initial_log_probs = tf.constant([[0.] + [-INF] * (beam_size - 1)])\n",
        "  # Expand to beam_size (batch_size, beam_size)\n",
        "  alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])\n",
        "\n",
        "  # Expand each batch and state to beam_size\n",
        "  alive_seq = _expand_to_beam_size(initial_ids, beam_size)\n",
        "  alive_seq = tf.expand_dims(alive_seq, axis=2)  # (batch_size, beam_size, 1)\n",
        "  if use_tpu:\n",
        "    alive_seq = tf.tile(alive_seq, [1, 1, decode_length + 1])\n",
        "  if states:\n",
        "    states = nest.map_structure(\n",
        "        lambda state: _expand_to_beam_size(state, beam_size), states)\n",
        "  else:\n",
        "    states = {}\n",
        "\n",
        "  # Finished will keep track of all the sequences that have finished so far\n",
        "  # Finished log probs will be negative infinity in the beginning\n",
        "  # finished_flags will keep track of booleans\n",
        "  finished_seq = tf.zeros(common_layers.shape_list(alive_seq), tf.int32)\n",
        "  # Setting the scores of the initial to negative infinity.\n",
        "  finished_scores = tf.ones([batch_size, beam_size]) * -INF\n",
        "  finished_flags = tf.zeros([batch_size, beam_size], tf.bool)\n",
        "\n",
        "  def grow_finished(finished_seq, finished_scores, finished_flags, curr_seq,\n",
        "                    curr_scores, curr_finished):\n",
        "    \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n",
        "\n",
        "    Args:\n",
        "      finished_seq: Current finished sequences.\n",
        "        [batch_size, beam_size, current_decoded_length]\n",
        "      finished_scores: scores for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      finished_flags: finished bools for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      curr_seq: current topk sequence that has been grown by one position.\n",
        "        [batch_size, beam_size, current_decoded_length]\n",
        "      curr_scores: scores for each of these sequences. [batch_size, beam_size]\n",
        "      curr_finished: Finished flags for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "    Returns:\n",
        "      Tuple of\n",
        "        (Topk sequences based on scores,\n",
        "         log probs of these sequences,\n",
        "         Finished flags of these sequences)\n",
        "    \"\"\"\n",
        "    if not use_tpu:\n",
        "      # First append a column of 0'ids to finished to make the same length with\n",
        "      # finished scores\n",
        "      finished_seq = tf.concat(\n",
        "          [finished_seq,\n",
        "           tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)\n",
        "\n",
        "    # Set the scores of the unfinished seq in curr_seq to large negative\n",
        "    # values\n",
        "    curr_scores += (1. - tf.cast(curr_finished, dtype=tf.float32)) * -INF\n",
        "    # concatenating the sequences and scores along beam axis\n",
        "    curr_finished_seq = tf.concat([finished_seq, curr_seq], axis=1)\n",
        "    curr_finished_scores = tf.concat([finished_scores, curr_scores], axis=1)\n",
        "    curr_finished_flags = tf.concat([finished_flags, curr_finished], axis=1)\n",
        "    return compute_topk_scores_and_seq(\n",
        "        curr_finished_seq, curr_finished_scores, curr_finished_scores,\n",
        "        curr_finished_flags, beam_size, batch_size, \"grow_finished\",\n",
        "        use_tpu=use_tpu)\n",
        "\n",
        "  def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished, states):\n",
        "    \"\"\"Given sequences and scores, will gather the top k=beam size sequences.\n",
        "\n",
        "    Args:\n",
        "      curr_seq: current topk sequence that has been grown by one position.\n",
        "        [batch_size, beam_size, i+1]\n",
        "      curr_scores: scores for each of these sequences. [batch_size, beam_size]\n",
        "      curr_log_probs: log probs for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      curr_finished: Finished flags for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      states: dict (possibly nested) of decoding states.\n",
        "    Returns:\n",
        "      Tuple of\n",
        "        (Topk sequences based on scores,\n",
        "         log probs of these sequences,\n",
        "         Finished flags of these sequences)\n",
        "    \"\"\"\n",
        "    # Set the scores of the finished seq in curr_seq to large negative\n",
        "    # values\n",
        "    curr_scores += tf.cast(curr_finished, dtype=tf.float32) * -INF\n",
        "    return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs,\n",
        "                                       curr_finished, beam_size, batch_size,\n",
        "                                       \"grow_alive\", states, use_tpu=use_tpu)\n",
        "\n",
        "  def grow_topk(i, alive_seq, alive_log_probs, states):\n",
        "    r\"\"\"Inner beam search loop.\n",
        "\n",
        "    This function takes the current alive sequences, and grows them to topk\n",
        "    sequences where k = 2*beam. We use 2*beam because, we could have beam_size\n",
        "    number of sequences that might hit <EOS> and there will be no alive\n",
        "    sequences to continue. With 2*beam_size, this will not happen. This relies\n",
        "    on the assumption the vocab size is > beam size. If this is true, we'll\n",
        "    have at least beam_size non <EOS> extensions if we extract the next top\n",
        "    2*beam words.\n",
        "    Length penalty is given by = (5+len(decode)/6) ^ -\\alpha. Pls refer to\n",
        "    https://arxiv.org/abs/1609.08144.\n",
        "\n",
        "    Args:\n",
        "      i: loop index\n",
        "      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]\n",
        "      alive_log_probs: probabilities of these sequences. [batch_size, beam_size]\n",
        "      states: dict (possibly nested) of decoding states.\n",
        "    Returns:\n",
        "      Tuple of\n",
        "        (Topk sequences extended by the next word,\n",
        "         The log probs of these sequences,\n",
        "         The scores with length penalty of these sequences,\n",
        "         Flags indicating which of these sequences have finished decoding,\n",
        "         dict of transformed decoding states)\n",
        "    \"\"\"\n",
        "    # Get the logits for all the possible next symbols\n",
        "    if use_tpu and states:\n",
        "      flat_ids = tf.reshape(\n",
        "          tf.slice(alive_seq, [0, 0, i], [batch_size, beam_size, 1]),\n",
        "          [batch_size * beam_size, -1])\n",
        "    else:\n",
        "      flat_ids = tf.reshape(alive_seq, [batch_size * beam_size, -1])\n",
        "\n",
        "    # (batch_size * beam_size, decoded_length)\n",
        "    if states:\n",
        "      flat_states = nest.map_structure(_merge_beam_dim, states)\n",
        "      flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i, flat_states)\n",
        "      states = nest.map_structure(\n",
        "          lambda t: _unmerge_beam_dim(t, batch_size, beam_size), flat_states)\n",
        "    elif use_tpu:\n",
        "      flat_logits = symbols_to_logits_fn(flat_ids, i)\n",
        "    else:\n",
        "      flat_logits = symbols_to_logits_fn(flat_ids)\n",
        "\n",
        "    logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])\n",
        "\n",
        "    # Convert logits to normalized log probs\n",
        "    candidate_log_probs = common_layers.log_prob_from_logits(logits)\n",
        "\n",
        "    # Multiply the probabilities by the current probabilities of the beam.\n",
        "    # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)\n",
        "    log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
        "\n",
        "    length_penalty = tf.pow(((5. + tf.cast(i + 1, dtype=tf.float32)) / 6.), alpha)\n",
        "\n",
        "    curr_scores = log_probs / length_penalty\n",
        "    # Flatten out (beam_size, vocab_size) probs in to a list of possibilities\n",
        "    flat_curr_scores = tf.reshape(curr_scores, [-1, beam_size * vocab_size])\n",
        "\n",
        "    if use_tpu:\n",
        "      topk_scores, topk_ids = top_k_with_unique(\n",
        "          flat_curr_scores, k=beam_size * 2)\n",
        "    else:\n",
        "      topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size * 2)\n",
        "\n",
        "    # Recovering the log probs because we will need to send them back\n",
        "    topk_log_probs = topk_scores * length_penalty\n",
        "\n",
        "    # Work out what beam the top probs are in.\n",
        "    topk_beam_index = topk_ids // vocab_size\n",
        "    topk_ids %= vocab_size  # Unflatten the ids\n",
        "\n",
        "    if not use_tpu:\n",
        "      # The next three steps are to create coordinates for tf.gather_nd to pull\n",
        "      # out the correct sequences from id's that we need to grow.\n",
        "      # We will also use the coordinates to gather the booleans of the beam\n",
        "      # items that survived.\n",
        "      batch_pos = compute_batch_indices(batch_size, beam_size * 2)\n",
        "\n",
        "      # top beams will give us the actual coordinates to do the gather.\n",
        "      # stacking will create a tensor of dimension batch * beam * 2, where the\n",
        "      # last dimension contains the i,j gathering coordinates.\n",
        "      topk_coordinates = tf.stack([batch_pos, topk_beam_index], axis=2)\n",
        "\n",
        "      # Gather up the most probable 2*beams both for the ids and\n",
        "      # finished_in_alive bools\n",
        "      topk_seq = tf.gather_nd(alive_seq, topk_coordinates)\n",
        "      if states:\n",
        "        states = nest.map_structure(\n",
        "            lambda state: tf.gather_nd(state, topk_coordinates), states)\n",
        "\n",
        "      # Append the most probable alive\n",
        "      topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)\n",
        "    else:\n",
        "      # Gather up the most probable 2*beams both for the ids and\n",
        "      # finished_in_alive bools\n",
        "      topk_seq = fast_tpu_gather(alive_seq, topk_beam_index)\n",
        "\n",
        "      if states:\n",
        "        states = nest.map_structure(\n",
        "            lambda state: fast_tpu_gather(state, topk_beam_index), states)\n",
        "\n",
        "      # Update the most probable alive\n",
        "      topk_seq = tf.transpose(a=topk_seq, perm=[2, 0, 1])\n",
        "      topk_seq = inplace_ops.alias_inplace_update(topk_seq, i + 1, topk_ids)\n",
        "      topk_seq = tf.transpose(a=topk_seq, perm=[1, 2, 0])\n",
        "\n",
        "    topk_finished = tf.equal(topk_ids, eos_id)\n",
        "\n",
        "    return topk_seq, topk_log_probs, topk_scores, topk_finished, states\n",
        "\n",
        "  def inner_loop(i, alive_seq, alive_log_probs, finished_seq, finished_scores,\n",
        "                 finished_flags, states):\n",
        "    \"\"\"Inner beam search loop.\n",
        "\n",
        "    There are three groups of tensors, alive, finished, and topk.\n",
        "    The alive group contains information about the current alive sequences\n",
        "    The topk group contains information about alive + topk current decoded words\n",
        "    the finished group contains information about finished sentences, that is,\n",
        "    the ones that have decoded to <EOS>. These are what we return.\n",
        "    The general beam search algorithm is as follows:\n",
        "    While we haven't terminated (pls look at termination condition)\n",
        "      1. Grow the current alive to get beam*2 topk sequences\n",
        "      2. Among the topk, keep the top beam_size ones that haven't reached EOS\n",
        "      into alive\n",
        "      3. Among the topk, keep the top beam_size ones have reached EOS into\n",
        "      finished\n",
        "    Repeat\n",
        "    To make things simple with using fixed size tensors, we will end\n",
        "    up inserting unfinished sequences into finished in the beginning. To stop\n",
        "    that we add -ve INF to the score of the unfinished sequence so that when a\n",
        "    true finished sequence does appear, it will have a higher score than all the\n",
        "    unfinished ones.\n",
        "\n",
        "    Args:\n",
        "      i: loop index\n",
        "      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]\n",
        "      alive_log_probs: probabilities of the beams. [batch_size, beam_size]\n",
        "      finished_seq: Current finished sequences.\n",
        "        [batch_size, beam_size, i+1]\n",
        "      finished_scores: scores for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      finished_flags: finished bools for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      states: dict (possibly nested) of decoding states.\n",
        "\n",
        "    Returns:\n",
        "      Tuple of\n",
        "        (Incremented loop index\n",
        "         New alive sequences,\n",
        "         Log probs of the alive sequences,\n",
        "         New finished sequences,\n",
        "         Scores of the new finished sequences,\n",
        "         Flags indicating which sequence in finished as reached EOS,\n",
        "         dict of final decoding states)\n",
        "    \"\"\"\n",
        "\n",
        "    # Each inner loop, we carry out three steps:\n",
        "    # 1. Get the current topk items.\n",
        "    # 2. Extract the ones that have finished and haven't finished\n",
        "    # 3. Recompute the contents of finished based on scores.\n",
        "    topk_seq, topk_log_probs, topk_scores, topk_finished, states = grow_topk(\n",
        "        i, alive_seq, alive_log_probs, states)\n",
        "    alive_seq, alive_log_probs, _, states = grow_alive(\n",
        "        topk_seq, topk_scores, topk_log_probs, topk_finished, states)\n",
        "    finished_seq, finished_scores, finished_flags, _ = grow_finished(\n",
        "        finished_seq, finished_scores, finished_flags, topk_seq, topk_scores,\n",
        "        topk_finished)\n",
        "\n",
        "    return (i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores,\n",
        "            finished_flags, states)\n",
        "\n",
        "  def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq,\n",
        "                   finished_scores, finished_in_finished, unused_states):\n",
        "    \"\"\"Checking termination condition.\n",
        "\n",
        "    We terminate when we decoded up to decode_length or the lowest scoring item\n",
        "    in finished has a greater score that the highest prob item in alive divided\n",
        "    by the max length penalty\n",
        "\n",
        "    Args:\n",
        "      i: loop index\n",
        "      alive_log_probs: probabilities of the beams. [batch_size, beam_size]\n",
        "      finished_scores: scores for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "      finished_in_finished: finished bools for each of these sequences.\n",
        "        [batch_size, beam_size]\n",
        "\n",
        "    Returns:\n",
        "      Bool.\n",
        "    \"\"\"\n",
        "    if not stop_early:\n",
        "      return tf.less(i, decode_length)\n",
        "    max_length_penalty = tf.pow(((5. + tf.cast(decode_length, dtype=tf.float32)) / 6.), alpha)\n",
        "    # The best possible score of the most likely alive sequence.\n",
        "    lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty\n",
        "\n",
        "    # Now to compute the lowest score of a finished sequence in finished\n",
        "    # If the sequence isn't finished, we multiply it's score by 0. since\n",
        "    # scores are all -ve, taking the min will give us the score of the lowest\n",
        "    # finished item.\n",
        "    lowest_score_of_finished_in_finished = tf.reduce_min(\n",
        "        input_tensor=finished_scores * tf.cast(finished_in_finished, dtype=tf.float32), axis=1)\n",
        "    # If none of the sequences have finished, then the min will be 0 and\n",
        "    # we have to replace it by -ve INF if it is. The score of any seq in alive\n",
        "    # will be much higher than -ve INF and the termination condition will not\n",
        "    # be met.\n",
        "    lowest_score_of_finished_in_finished += (\n",
        "        (1. - tf.cast(tf.reduce_any(input_tensor=finished_in_finished, axis=1), dtype=tf.float32)) * -INF)\n",
        "\n",
        "    bound_is_met = tf.reduce_all(\n",
        "        input_tensor=tf.greater(lowest_score_of_finished_in_finished,\n",
        "                   lower_bound_alive_scores))\n",
        "\n",
        "    return tf.logical_and(\n",
        "        tf.less(i, decode_length), tf.logical_not(bound_is_met))\n",
        "\n",
        "  (_, alive_seq, alive_log_probs, finished_seq, finished_scores,\n",
        "   finished_flags, _) = tf.while_loop(\n",
        "       cond=_is_finished,\n",
        "       body=inner_loop, loop_vars=[\n",
        "           tf.constant(0), alive_seq, alive_log_probs, finished_seq,\n",
        "           finished_scores, finished_flags, states\n",
        "       ],\n",
        "       shape_invariants=[\n",
        "           tf.TensorShape([]),\n",
        "           (tf.TensorShape([batch_size, beam_size, decode_length + 1])\n",
        "            if use_tpu else tf.TensorShape([None, None, None])),\n",
        "           alive_log_probs.get_shape(),\n",
        "           (tf.TensorShape([batch_size, beam_size, decode_length + 1])\n",
        "            if use_tpu else tf.TensorShape([None, None, None])),\n",
        "           finished_scores.get_shape(),\n",
        "           finished_flags.get_shape(),\n",
        "           (nest.map_structure(lambda state: state.get_shape(), states)\n",
        "            if use_tpu else\n",
        "            nest.map_structure(get_state_shape_invariants, states)),\n",
        "       ],\n",
        "       parallel_iterations=1,\n",
        "       back_prop=False)\n",
        "\n",
        "  alive_seq.set_shape((None, beam_size, None))\n",
        "  finished_seq.set_shape((None, beam_size, None))\n",
        "\n",
        "  # Accounting for corner case: It's possible that no sequence in alive for a\n",
        "  # particular batch item ever reached EOS. In that case, we should just copy\n",
        "  # the contents of alive for that batch item. tf.reduce_any(finished_flags, 1)\n",
        "  # if 0, means that no sequence for that batch index had reached EOS. We need\n",
        "  # to do the same for the scores as well.\n",
        "  finished_seq = tf.compat.v1.where(\n",
        "      tf.reduce_any(input_tensor=finished_flags, axis=1), finished_seq, alive_seq)\n",
        "  finished_scores = tf.compat.v1.where(\n",
        "      tf.reduce_any(input_tensor=finished_flags, axis=1), finished_scores, alive_log_probs)\n",
        "  return finished_seq, finished_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p-f6PNwTARz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_size = 7\n",
        "batch_size = 1\n",
        "inp_sentence = \"you should do well\"\n",
        "greed_ip = inp_sentence\n",
        "start_token = [tokenizer_en.vocab_size]\n",
        "end_token = [tokenizer_en.vocab_size + 1]\n",
        "inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "encoder_input = tf.concat([encoder_input]*beam_size, axis=0)\n",
        "def transformer_query(output):\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  predictions, attention_weights = transformer(encoder_input, \n",
        "                                               output,\n",
        "                                               False,\n",
        "                                               enc_padding_mask,\n",
        "                                               combined_mask,\n",
        "                                               dec_padding_mask)\n",
        " \n",
        "\n",
        "  return (predictions[:,-1:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4rS2dG0xl8",
        "colab_type": "code",
        "outputId": "a3b5649d-26cb-4842-b83b-ea5aab3de588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = tokenizer_ta.vocab_size\n",
        "end = tokenizer_ta.vocab_size+1\n",
        "\n",
        "#initial_ids = tf.constant([start])\n",
        "a=beam_search(transformer_query, [start], beam_size, 50, target_vocab_size,0.6,eos_id=[tokenizer_ta.vocab_size+1])\n",
        "\n",
        "print(tokenizer_ta.decode([j for j in a[0][0][0].numpy() if j < tokenizer_ta.vocab_size]))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "நீங்கள் செய்ய வேண்டும்\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaHBrumbG2aU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e83afbf0-f7b7-4d1a-8ce6-3e5d879cb135"
      },
      "source": [
        "translate(greed_ip)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "Input: you should do well\n",
            "Predicted translation: நீங்கள் சரியான செய்ய வேண்டும்\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-IO_DCaXgzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXphuoTG8_Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def beam_search(prev, i):\n",
        "#   batch_size = 1\n",
        "#   beam_size = 3  # Number of hypotheses in beam\n",
        "#   vocab_size = target_vocab_size  # tamil vocabulary size\n",
        "#   num_steps = MAX_LENGTH # MAX_LENGTH\n",
        "#   embedding_size = d_model#dmodel 256\n",
        "#   embedding = transformer.decoder.get_weights()[0]  \n",
        "#   W =  tf.random.normal([embedding_size, vocab_size], mean=2, stddev=1, dtype=tf.float32, seed=0)\n",
        "#   b =  tf.random.normal([vocab_size], mean=0, stddev=1, dtype=tf.float32, seed=0)\n",
        "#   index_base = tf.reshape(\n",
        "#     tf.tile(tf.expand_dims(tf.range(batch_size) * beam_size, axis=1), [1, beam_size]), [-1])\n",
        "#   log_beam_probs, beam_symbols = [], []\n",
        "#   if i > 1:# total probability\n",
        "#     prev = tf.compat.v1.nn.xw_plus_b(prev, W, b)\n",
        "#     log_probs = tf.nn.log_softmax(prev) \n",
        "#     log_probs = tf.reshape(tf.reduce_sum(input_tensor=tf.stack(log_beam_probs, axis=1), axis=1) + log_probs,\n",
        "# \t\t\t\t\t\t\t   [-1, beam_size * vocab_size])\n",
        "#     # (batch_size*beam_size, vocab_size) -> (batch_size, beam_size*vocab_size)\n",
        "#   else:\n",
        "#     log_probs = tf.nn.log_softmax(prev)\n",
        "    \n",
        "#   best_probs, indices = tf.nn.top_k(log_probs, beam_size)\n",
        "#   # (batch_size, beam_size)\n",
        "#   indices = tf.squeeze(tf.reshape(indices, [-1, 1]))\n",
        "#   best_probs = tf.reshape(best_probs, [-1, 1])\n",
        "#   #print()\n",
        "#   # (batch_size*beam_size)\n",
        "#   symbols = indices % vocab_size       # which word in vocabulary\n",
        "#   beam_parent = indices // vocab_size  # which hypothesis it came from\n",
        "#   beam_symbols.append(symbols)\n",
        "#   #print(log_probs)\n",
        "#   real_path = beam_parent + index_base\n",
        "#   # get rid of the previous probability\n",
        "#   if i > 1:\n",
        "#     pre_sum = tf.reduce_sum(input_tensor=tf.stack(log_beam_probs, axis=1), axis=1)\n",
        "#     pre_sum = tf.gather(pre_sum, real_path)\n",
        "#   else:\n",
        "#     pre_sum = 0\n",
        "#   log_beam_probs.append(best_probs-pre_sum)\n",
        "#   if i > 1:\n",
        "#     for j in range(i)[:0:-1]:\n",
        "#       beam_symbols[j-1] = tf.gather(beam_symbols[j-1], real_path)\n",
        "#       log_beam_probs[j-1] = tf.gather(log_beam_probs[j-1], real_path)\n",
        "#   return tf.nn.embedding_lookup(params=embedding, ids=symbols), beam_symbols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyVgcxhE9a76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def evaluate(inp_sentence):\n",
        "#   start_token = [tokenizer_en.vocab_size]\n",
        "#   end_token = [tokenizer_en.vocab_size + 1]\n",
        "  \n",
        "#   # inp sentence is tamil, hence adding the start and end token\n",
        "#   inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "#   encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "#   # as the target is english, the first word to the transformer should be the\n",
        "#   # english start token.\n",
        "#   decoder_input = [tokenizer_ta.vocab_size]\n",
        "#   output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "#   for i in range(MAX_LENGTH):\n",
        "#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "#         encoder_input, output)\n",
        "  \n",
        "#     # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "#     predictions, attention_weights = transformer(encoder_input, \n",
        "#                                                  output,\n",
        "#                                                  False,\n",
        "#                                                  enc_padding_mask,\n",
        "#                                                  combined_mask,\n",
        "#                                                  dec_padding_mask)\n",
        "#     predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "#     #print('predictions')\n",
        "#     #print(predictions)\n",
        "#     if i==0:\n",
        "#       bpredictions = predictions\n",
        "#     bpredictions,beam_symbols = beam_search(bpredictions, i+1)  #next_input should be predictions(transformer op) (logits)\n",
        "#     #print(best_seq)\n",
        "#     seq_rank = tf.stack(values=beam_symbols)\n",
        "\n",
        "#   return seq_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XCiZ2RXWxW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}